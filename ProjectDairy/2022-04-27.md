Det här med att faktiskt läsa dokumentation istället för att bara anta att man vet vad saker gör är något som tror det eller ej är rätt bra att göra. Det stora problemet jag fått nu har att göra
med hur last write time för directories. Först antog att jag last write time för ett directory var något som uppdateras givet att något annat under directoryt uppdaterades. Så är inte fallet.
Tydligen uppdateras dem enbart då filer eller subdirectorys läggs till till den. När jag tänker efter är det inte för orimligt. Att ett filsystem skulle uppdatera directorys givet att något under
ändrades måste nödvändigen innebära att för varje file write så uppdateras *alla* parent directorys, och även om jag inte är helt säker på exakt hur filsystem fungerar skulle detta kunna innebära så många
extra seek operationer, vilket inte känns helt effektivt. Men då kommer vi tillbaka till frågan, går det att uppdatera ett index för en lokal directory på ett bra sätt? Att last write time ser till
att man inte behöver hasha om alla filer är ju onekligen en stor performance boost, men att last write time inte är rekursiv måste innebära att för att uppdatera indexet för en directory
innebär att man åtminstone kollar last write time för *alla* filer som är en del av det. 

Men som med allt när det kommer till performance, det ända sättet att avgöra hur snabbt det är genom att faktiskt testa. Jag skrev därför ett enkelt program som gick igenom alla filer i ett directory
och accessade när last write time. För både assimp och ffmpeg, packet som tradditionellt tagit långt tid att uppdatera, gick på ingen tid alls. 11757 filer, alla filer i ffmpeg, kunde checkas
på 0.046 sekunder. Om detta har att göra med att jag har en ssd eller en snabb dator kan jag ju inte med enbart denna data avgöra, men oavsett vad visar det att rädslan jag har för att 
performancen ska tanka inte nödvändigtvis är berättigad. Däremot finns det fortfarande något väldigt otillfredsställande med det.

Fördelen med att directorys skulle kunna hålla koll på alla directorys under är att man får, givet att alla filer inte är i ett directory, log(n) operationer för att avgöra huruvida en godtycklig 
fil har uppdateras, där n är antalet filer i subdirectoryn med vissa antaganden. Med det nuvarande systemt däremot, måste alla filer gås igenom. Även fast det förmodligen i praktiken inte 
kommer spela roll finns det fortfarande den där känslan av att ett bauta packet kommer ta en stund att uppdatera för varje gång man uppdaterar. Men problemet har också framförallt att göra med
när man kollar igenom flera packets. Även om vi multiplicerar ffmpeg, det största indexet, med 20 så kommer vi ändå bara upp i 1 sekund. Men det kan heller aldrig bli lägre.

Jag funderate en kort stund på huruvida man kan ha indexar i subdirectorys som man uppdaterar separat, och även om det går i sig, är problemet att man behöver lita på att dem uppdaterats korrekt. 
Att faktist verfiera att allt är som det ska kommer oavsett vad kräva en checking av alla filer. 